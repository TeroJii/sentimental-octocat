---
title: "Analyzing sentiments"
author: "Tero Jalkanen"
date: "`r Sys.Date()`"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

## Packages -----------

library(readxl)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(tidytext)

## Set file paths
here::i_am("text_analysis.Rmd")

## Set graphical theme
theme_set(theme_minimal())

```

## Data 

* The data contains 266 sentences which have been assessed as being `positive`, `negative` or `neutral`.

* We want to explore the data and see if it is useful for sentiment analysis.

## General properties of the data

* There are no missing entries

```{r}

text_df <- readxl::read_excel(path = here("data", "sentences_with_sentiment.xlsx")) %>% 
  # move sentiments under same column
  mutate(sentiment = case_when(
    Positive == 1 ~ "Positive",
    Negative == 1 ~ "Negative",
    Neutral == 1 ~ "Neutral",
    # in case of missing sentiment
    TRUE ~ "missing"
  ), .after = ID) %>% 
  janitor::clean_names()

# Let's take a look at the data
skim(text_df)

```

## The balance of sentiments


```{r}

text_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(prercentage = 100*n/length(text_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")

```

## Unique sentences

Check if there are duplicate sentences.

```{r}
## Find duplicate sentences ----

# indices
double_ind <- duplicated(text_df$sentence)

# The actual sentences
doubles <- text_df$sentence[double_ind] %>% 
  unique()

```

* There are `r length(doubles)` sentences which appear at least twice.

```{r}

# Let's print out the first example
text_df %>% 
  filter(sentence %in% doubles[1]) %>% 
  select(sentiment, sentence)

```

## Duplicated sentences with differing sentiment label

Are there duplicated sentences, where the sentence has been labelled differently?

```{r}

doubles_df <- data.frame(sentence = doubles, double_id = 1:length(doubles))

text_df %>% 
  filter(sentence %in% doubles) %>% 
  left_join(doubles_df, by = "sentence") %>% 
  group_by(double_id, sentiment) %>% 
  summarise(n = n())

```

## Things to consider

* Should we use the duplicated sentences only once?

* How to deal with the imbalance between sentiments?

* Evaluating classification performance?

# Extracting words as tokens

## Extracting words as tokens

* Use words, as tokens, everything in small letters, remove punctuation, stopwords etc.

```{r}
# The first sentence in the data
print(text_df$sentence[1])

# A tidy version with words as tokens
(tidy_df <- text_df %>% 
  unnest_tokens(word, sentence))

```

## Removing stopwords

```{r}

data("stop_words")

(tidy_df <- tidy_df %>% 
  anti_join(stop_words))

```

## Most common words

```{r}
tidy_df %>% 
  count(word, sort = TRUE) %>% 
  head(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most common words in the example sentences",
          subtitle = "Excluding common stopwords")
```



## Slide with R Output

```{r cars, echo = TRUE}
summary(cars)
```

## Slide with Plot

```{r pressure}
plot(pressure)
```

