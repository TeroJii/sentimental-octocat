---
title: "Analyzing sentiments"
author: "Tero Jalkanen"
date: "`r Sys.Date()`"
output: ioslides_presentation
params:
  filter_duplicates: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

## Packages -----------

library(readxl)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(tidytext)
library(scales)
library(textdata)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(tidymodels)

## Set file paths
here::i_am("text_analysis.Rmd")

## Set graphical theme
theme_set(theme_minimal())

```

## Data 

* The data contains 266 sentences written in English which have been assessed as being `positive`, `negative` or `neutral`.

* We want to explore the data and see if it is useful for sentiment analysis.

## General properties of the data

* There are no missing entries

```{r}

text_df <- readxl::read_excel(path = here("data", "sentences_with_sentiment.xlsx")) %>% 
  # move sentiments under same column
  mutate(sentiment = case_when(
    Positive == 1 ~ "Positive",
    Negative == 1 ~ "Negative",
    Neutral == 1 ~ "Neutral",
    # in case of missing sentiment
    TRUE ~ "missing"
  ), .after = ID) %>% 
  janitor::clean_names()

# Let's take a look at the data
skim(text_df)

```

## The balance of sentiments


```{r}

text_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(prercentage = 100*n/length(text_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")

```

## Unique sentences

Check if there are duplicate sentences.

```{r}
## Find duplicate sentences ----

# indices
double_ind <- duplicated(text_df$sentence)

# The actual sentences
doubles <- text_df$sentence[double_ind] %>% 
  unique()

```

* There are `r length(doubles)` sentences which appear at least twice.

```{r}

# Let's print out the first example
text_df %>% 
  filter(sentence %in% doubles[1]) %>% 
  select(sentiment, sentence)


```

## Duplicated sentences with differing sentiment label

Are there duplicated sentences, where the sentence has been labelled differently?

```{r}

doubles_df <- data.frame(sentence = doubles, double_id = 1:length(doubles))

text_df %>% 
  filter(sentence %in% doubles) %>% 
  left_join(doubles_df, by = "sentence") %>% 
  group_by(double_id, sentiment) %>% 
  summarise(n = n())

```

## What happens to sentiment balance without duplicates?

```{r}
# sentiments without duplicates
no_doubles_df <- text_df %>%
  select(-id) %>% 
  # include doubles only once
  unique.data.frame()


#balance table
no_doubles_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(prercentage = 100*n/length(no_doubles_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")


## Filter out duplicate sentences if the filtering parameter is set to TRUE
if(params$filter_duplicates){
  # remove duplicate entiries
  # each sentence only appears once
  text_df <- text_df[!double_ind,]
}


```


## Things to consider

* Should we use the duplicated sentences only once?

  - Having the same sentence with the same sentiment label twice does not add new information

  - Added a parameter to filter out duplicate sentences

* How to deal with the imbalance between sentiments?

* Evaluating classification performance?

# Extracting words as tokens

## Extracting words as tokens

* Use words, as tokens, everything in small letters, remove punctuation, stopwords etc.

```{r}
# The first sentence in the data
print(text_df$sentence[1])

# A tidy version with words as tokens
(tidy_df <- text_df %>% 
  unnest_tokens(word, sentence))

```

## Removing stopwords

* Note that there might be big differences between different precompiled stopword lists

```{r}

# Get a dictionary of common stopwords which ship with tidytext
data("stop_words")

(tidy_df <- tidy_df %>% 
  anti_join(stop_words))

```

## Most common words

```{r}
tidy_df %>% 
  count(word, sort = TRUE) %>% 
  head(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most common words in the example sentences",
          subtitle = "Excluding common stopwords")
```


## Words vs. different sentiments


```{r, fig.height=7, fig.width=8, warning=FALSE}

tidy_df %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  # twenty most common words per sentiment
  slice(1:20) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"),
                   max.words = 60)

```


## Word Proportions in Negative/Neural vs. Positive sentences

```{r, warning=FALSE}

tidy_df %>% 
  count(sentiment, word) %>% 
  group_by(sentiment) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = sentiment, values_from = proportion) %>% 
  pivot_longer(Negative:Neutral, names_to = "sentiment", values_to = "proportion") %>% 
  ggplot(aes(x = proportion, y = Positive, color = abs(Positive - proportion))) +
  geom_abline(lty = 2) +
  geom_point(size = 2.5, alpha = 0.5) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_grid(sentiment ~ .) +
  labs(x = "Negative (top) / Neutral (bottom)") +
  theme(legend.position = "none")
  
  

```


# Attaching sentiments to individual words

## Sentiments from dictionaries

* A number of readily available sentiment dictionaries are available, which help us attach emotion to single words

> Dictionary-based methods find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.

* Example ([AFINN](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) dictionary by Finn Årup Nielsen):

```{r}

get_sentiments("afinn") %>% 
  head(3)

```

## Attach sentiment to sentences

```{r}

tidy_df %>% 
  # Add sentiment scores for words from afinn dictionary 
  inner_join(get_sentiments("afinn")) %>% 
  count(id, sentiment, value) %>% 
  group_by(id, sentiment) %>% 
  summarise(sentiment_score = sum(value*n)) %>% 
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Neutral", "Negative"))) %>% 
  ggplot(aes(x = id, y = sentiment_score, fill = sentiment)) +
  geom_col() +
  ggtitle("Sentiments from AFINN dictionary by Finn Årup Nielsen") +
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  labs(y = "Sentiment scores for sentences", x = "Sentence ID")

```

* Drawbacks such as many words are missing from this dictionary

* Sentiments depend on the context:

> Tero likes to **share** with his friends.
> The OmniCorp **share** went into sharp decline.

## TF-IDF

```{r}

sentiments_td_idf <- tidy_df %>%
  group_by(sentiment) %>% 
  count(word) %>%
  #filter out numerics
  filter(!grepl(pattern = "[0-9]+", x = word)) %>%
  bind_tf_idf(word, sentiment, n)

sentiments_td_idf %>% 
  group_by(sentiment) %>% 
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, ncol = 1, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```

## Simple model with the TF-IDF

```{r}

# set.seed(123)
# # make 80/20 train/test split
# # Stratify according to sentiment
# # remove one hot encoding
# split <- initial_split(select(text_df, -c("positive", "negative", "neutral")), prop = 0.8, strata = "sentiment")
# 
# train_data <- training(split)
# test_data <- testing(split)
# 
# feature_set <- feature_set(text)
# 
# text_df %>% head()
  

```



# n-grams instead of words

## Bigrams

* Bigrams are two words that appear in succession

* Most of the common bigrams are unsurprizingly composed of stopwords

```{r}

# Find bigrams
bigrams_df <- text_df %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

```


```{r}

bigrams_df %>% 
  count(bigram, sort = TRUE)

```

## Removing stopwords from bigrams

```{r}

bigrams_separated <- bigrams_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

```

## Finding weird combinations

```{r}

text_df %>% 
  filter(grepl(pattern = "sof", x = sentence, ignore.case =TRUE)) %>% 
  select(sentence)

```

## 

```{r}

# Unite bigrams after removing stopwords
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count TF-IDF for bigrams 
bigram_tf_idf <- bigrams_united %>%
  count(sentiment, bigram) %>%
  bind_tf_idf(bigram, sentiment, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>% 
  group_by(sentiment) %>% 
  arrange(desc(tf_idf)) %>%
  top_n(3) %>% 
  ggplot(aes(x = tf_idf, y = bigram, fill = sentiment)) +
  geom_col() +
  facet_grid(sentiment~., scales = "free")

```

## Most common negations

```{r}

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

```



## Latent Dirichlet Allocation


# Classification

## Test/train-split

```{r}

# Make test/train split 20/80
# Same fraction of sentiments in both sets
text_split <- initial_split(text_df, prop = 0.8, strata = sentiment)

text_train <- training(text_split)
text_test <- testing(text_split)

dim(text_train)
dim(text_test)
```



# Extra slides

## Numeric "words" by sentiment

```{r}
## Numeric information by sentiment
tidy_df %>%
  group_by(sentiment) %>% 
  count(word) %>%
  #filter out numerics
  filter(grepl(pattern = "[0-9]+", x = word)) %>%
  group_by(sentiment) %>% 
  slice(1:10) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"))

```

