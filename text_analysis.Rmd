---
title: "Analyzing sentiments"
author: "Tero Jalkanen"
date: "`r Sys.Date()`"
output: ioslides_presentation
params:
  filter_duplicates: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE)

## Packages -----------

library(readxl)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(tidytext)
library(scales)
library(textdata)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(tidymodels)
library(topicmodels)
library(textrecipes)
# methods for imbalanced data sets
library(themis)
library(hardhat)
library(glmnet)

## Set file paths
here::i_am("text_analysis.Rmd")

## Set graphical theme
theme_set(theme_minimal())

```

## Data 

* The data contains 266 sentences written in English which have been assessed as being `positive`, `negative` or `neutral`.

* We want to explore the data and see if it is useful for sentiment analysis.

## General properties of the data

* There are no missing entries

```{r}

text_df <- readxl::read_excel(path = here("data", "sentences_with_sentiment.xlsx")) %>% 
  # move sentiments under same column
  mutate(sentiment = case_when(
    Positive == 1 ~ "Positive",
    Negative == 1 ~ "Negative",
    Neutral == 1 ~ "Neutral",
    # in case of missing sentiment
    TRUE ~ "missing"
  ), .after = ID) %>% 
  janitor::clean_names()

# Let's take a look at the data
skim(text_df)

```

## The balance of sentiments


```{r}

text_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(prercentage = 100*n/length(text_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")

```

## Unique sentences

Check if there are duplicate sentences.

```{r}
## Find duplicate sentences ----

# indices
double_ind <- duplicated(text_df$sentence)

# The actual sentences
doubles <- text_df$sentence[double_ind] %>% 
  unique()

```

* There are `r length(doubles)` sentences which appear at least twice.

```{r}

# Let's print out the first example
text_df %>% 
  filter(sentence %in% doubles[1]) %>% 
  select(sentiment, sentence)


```

## Duplicated sentences with differing sentiment label

Are there duplicated sentences, where the sentence has been labelled differently?

```{r}

doubles_df <- data.frame(sentence = doubles, double_id = 1:length(doubles))

text_df %>% 
  filter(sentence %in% doubles) %>% 
  left_join(doubles_df, by = "sentence") %>% 
  group_by(double_id, sentiment) %>% 
  summarise(n = n())

```

## What happens to sentiment balance without duplicates?

```{r}
# sentiments without duplicates
no_doubles_df <- text_df %>%
  select(-id) %>% 
  # include doubles only once
  unique.data.frame()


#balance table
no_doubles_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(prercentage = 100*n/length(no_doubles_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")


## Filter out duplicate sentences if the filtering parameter is set to TRUE
if(params$filter_duplicates){
  # remove duplicate entiries
  # each sentence only appears once
  text_df <- text_df[!double_ind,]
}


```


## Things to consider

* Should we use the duplicated sentences only once?

  - Having the same sentence with the same sentiment label twice does not add new information

  - Added a parameter to filter out duplicate sentences

* How to deal with the imbalance between sentiments?

* Evaluating classification performance?

# Extracting words as tokens

## Extracting words as tokens

* Use words, as tokens, everything in small letters, remove punctuation, stopwords etc.

```{r}
# The first sentence in the data
print(text_df$sentence[1])

# A tidy version with words as tokens
(tidy_df <- text_df %>% 
  unnest_tokens(word, sentence))

```

## Removing stopwords

* Note that there might be big differences between different precompiled stopword lists

```{r}

# Get a dictionary of common stopwords which ship with tidytext
data("stop_words")

(tidy_df <- tidy_df %>% 
  anti_join(stop_words))

```

## Most common words

```{r}
tidy_df %>% 
  count(word, sort = TRUE) %>% 
  head(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most common words in the example sentences",
          subtitle = "Excluding common stopwords")
```


## Words vs. different sentiments


```{r, fig.height=7, fig.width=8, warning=FALSE}

tidy_df %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  # twenty most common words per sentiment
  slice(1:20) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"),
                   max.words = 60)

```


## Word Proportions in Negative/Neural vs. Positive sentences

```{r, warning=FALSE}

tidy_df %>% 
  count(sentiment, word) %>% 
  group_by(sentiment) %>% 
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  pivot_wider(names_from = sentiment, values_from = proportion) %>% 
  pivot_longer(Negative:Neutral, names_to = "sentiment", values_to = "proportion") %>% 
  ggplot(aes(x = proportion, y = Positive, color = abs(Positive - proportion))) +
  geom_abline(lty = 2) +
  geom_point(size = 2.5, alpha = 0.5) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_grid(sentiment ~ .) +
  labs(x = "Negative (top) / Neutral (bottom)") +
  theme(legend.position = "none")
  
  

```


# Attaching sentiments to individual words

## Sentiments from dictionaries

* A number of readily available sentiment dictionaries are available, which help us attach emotion to single words

> Dictionary-based methods find the total sentiment of a piece of text by adding up the individual sentiment scores for each word in the text.

* Example ([AFINN](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) dictionary by Finn Årup Nielsen):

```{r}

get_sentiments("afinn") %>% 
  head(3)

```

## Attach sentiment to sentences

```{r}

tidy_df %>% 
  # Add sentiment scores for words from afinn dictionary 
  inner_join(get_sentiments("afinn")) %>% 
  count(id, sentiment, value) %>% 
  group_by(id, sentiment) %>% 
  summarise(sentiment_score = sum(value*n)) %>% 
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral"))) %>% 
  ggplot(aes(x = id, y = sentiment_score, fill = sentiment)) +
  geom_col() +
  ggtitle("Sentiments from AFINN dictionary by Finn Årup Nielsen") +
  # Change plot colors
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  labs(y = "Sentiment scores for sentences", x = "Sentence ID")

```

* Drawbacks such as many words are missing from this dictionary

* Sentiments depend on the context:

> Tero likes to **share** with his friends.
> The OmniCorp **share** went into sharp decline.

## TF-IDF

```{r}

sentiments_td_idf <- tidy_df %>%
  group_by(sentiment) %>% 
  count(word) %>%
  #filter out numerics
  filter(!grepl(pattern = "[0-9]+", x = word)) %>%
  bind_tf_idf(word, sentiment, n)

sentiments_td_idf %>% 
  group_by(sentiment) %>% 
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral"))) %>%
  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  facet_wrap(~sentiment, ncol = 1, scales = "free") +
  labs(x = "tf-idf", y = NULL)
```



# n-grams instead of words

## Bigrams

* Bigrams are two words that appear in succession

* Most of the common bigrams are unsurprizingly composed of stopwords

```{r}

# Find bigrams
bigrams_df <- text_df %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

```


```{r}

bigrams_df %>% 
  count(bigram, sort = TRUE)

```

## Removing stopwords from bigrams

```{r}

bigrams_separated <- bigrams_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

bigram_counts

```

## Finding weird combinations

```{r}

text_df %>% 
  filter(grepl(pattern = "sof", x = sentence, ignore.case =TRUE)) %>% 
  select(sentence)

```

## 

```{r}

# Unite bigrams after removing stopwords
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count TF-IDF for bigrams 
bigram_tf_idf <- bigrams_united %>%
  count(sentiment, bigram) %>%
  bind_tf_idf(bigram, sentiment, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>% 
  group_by(sentiment) %>% 
  arrange(desc(tf_idf)) %>%
  top_n(3) %>% 
  ggplot(aes(x = tf_idf, y = bigram, fill = sentiment)) +
  geom_col() +
  facet_grid(sentiment~., scales = "free")

```

## Most common negations

```{r}

bigrams_separated %>%
  filter(word1 == "not") %>%
  count(word1, word2, sort = TRUE)

```

# Topic Modeling


## Latent Dirichlet Allocation


* Every document is a mixture of topics. We imagine that each document may contain words from several topics in particular proportions. For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”
    
* Every topic is a mixture of words. For example, we could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally.

* LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.

## LDA for sentences

* modify sentences to Document Term Matrix

```{r}
## NEEDS FIXING

# 
# # create document term matrix
# #sentences_dtm <- tidy_df %>% 
# tidy_df %>% 
#   count(id, sentiment, word) %>% 
#   cast_dtm(document = id, term = word, value = count)
#   
```


# Classification

## Test/train-split

```{r}

# Make test/train split 20/80
# Same fraction of sentiments in both sets
text_split <- initial_split(text_df, prop = 0.8, strata = sentiment)

text_train <- training(text_split)
text_test <- testing(text_split)

dim(text_train)
dim(text_test)
```

## Building a classification model

```{r}
# a recipe for classification
sentiment_rec <-
  recipe(sentiment ~ sentence, data = text_train)

## Add processing steps
sentiment_rec <- sentiment_rec %>% 
  # Extract words as tokens
  step_tokenize(sentence) %>% 
  # Only keep the 1000 most common tokens
  step_tokenfilter(sentence, max_tokens = 1000) %>%
  # Calculate TF-IDF
  step_tfidf(sentence)

## Add pre-processing steps to a workflow
## Ensures that test data will undergo similar pre-treatment steps without data leakage
sentiment_wf <- workflow() %>%
  add_recipe(sentiment_rec)


```

## 5-fold cross-validation

```{r}

## k-fold cross-validation
set.seed(234)
sentiment_folds <- vfold_cv(text_train, v = 5)

sentiment_folds

```



## Null model

* Performance when predicting the largest sentiment for all sentences

```{r}

null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

null_rs <- workflow() %>%
  add_recipe(sentiment_rec) %>%
  add_model(null_classification) %>%
  fit_resamples(
    sentiment_folds
  )

null_rs %>%
  collect_metrics()

```

## Confusion matrix null model

```{r}

# "Train" null model with entire train data and plot confusion matrix
workflow() %>%
  add_recipe(sentiment_rec) %>%
  add_model(null_classification) %>%
  # Fit null model for entire train data
  fit(data = text_train) %>% 
  # find predictions
  augment(text_train) %>%  
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Neutral", "Negative")),
         .pred_class = factor(.pred_class, levels = c("Positive", "Neutral", "Negative"))) %>% 
  conf_mat(sentiment, .pred_class) %>% 
  autoplot("heatmap") +
  ggtitle("Null model predictions on the train data")

```


## First model (LASSO)


```{r}

multi_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

multi_spec

### ----

sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")


multi_lasso_wf <- workflow() %>%
  add_recipe(sentiment_rec, blueprint = sparse_bp) %>%
  add_model(multi_spec)

multi_lasso_wf
```

## Tuning parameters

```{r}

# Regularization parameter lambda
smaller_lambda <- grid_regular(penalty(range = c(-5, 0)), levels = 20)
lambda_grid <- grid_regular(penalty(), levels = 30)



set.seed(2020)
multi_lasso_rs <- tune_grid(
  multi_lasso_wf,
  sentiment_folds,
#  grid = smaller_lambda,
  grid = lambda_grid,
  control = control_resamples(save_pred = TRUE)
)


best_acc <- multi_lasso_rs %>%
  show_best("accuracy")

best_acc

best_auc <- multi_lasso_rs %>%
  show_best("roc_auc")

best_auc
```

## Choosing the best value for lambda

```{r}

autoplot(multi_lasso_rs) +
  geom_vline(xintercept = best_acc$penalty[1], color = "red") +
  ggtitle("Regularization parameter with best accuracy")

```



## Confusion matrix

```{r}

multi_lasso_rs %>%
  collect_predictions() %>%
  filter(penalty == best_acc$penalty[1]) %>%
  filter(id == "Fold1") %>% 
  conf_mat(sentiment, .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("Example confusion matrix for Fold 1")

```

## Confusion matrix (best ROC AUC)


```{r}
multi_lasso_rs %>%
  collect_predictions() %>%
  filter(penalty == best_auc$penalty[1]) %>%
  filter(id == "Fold1") %>% 
  conf_mat(sentiment, .pred_class) %>%
  autoplot(type = "heatmap") +
  ggtitle("Example confusion matrix for Fold 1")

```

## What does the Lasso model eat

* Let's take a look at some of the predictors the model uses

* A large sparse matrix (188 rows: 1 per sentence x 1001 columns: sentiment + 1000 most common tokens)

```{r}

## Object fed to the LAsso model for training
sentiment_obj <- sentiment_rec %>%
  prep()
   
#str(bake(sentiment_obj, text_train))

## Visualizing object structure
bake(sentiment_obj, text_train) %>% 
  # Five first columns
  select(1:5) %>%
  # Five first rows
  slice(1:5) %>% 
  # Remove "_sentence" from predictor column names
  rename_with(~str_remove(., '_sentence')) %>%
  knitr::kable(caption = "Five first rows and columns")
```

## Numeric "words" by sentiment

```{r}
## Numeric information by sentiment
tidy_df %>%
  group_by(sentiment) %>% 
  count(word) %>%
  #filter out numerics
  filter(grepl(pattern = "[0-9]+", x = word)) %>%
  group_by(sentiment) %>% 
  slice(1:10) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"))

```



## We should try removing numbers from the model

* Remove stopwords

* Remove numerals

* Remove custom stopwords

## Capturing ambiguity

* Show probabilities for different sentiments

```{r}

## Set best lambda from k-fold when training model with full train data
multi_spec <- multinom_reg(penalty = best_acc$penalty[1], mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")

## Fit same model for entire train data
workflow() %>%
  add_recipe(sentiment_rec, blueprint = sparse_bp) %>%
  add_model(multi_spec) %>% 
  fit(data = text_train) %>% 
  # find predictions
  augment(text_train) %>%  
  # Extract predicted sentiment probabilities
  select(id, sentence, sentiment, .pred_Negative, .pred_Neutral, .pred_Positive) %>% 
  pivot_longer(cols = contains(".pred_"), names_to = "pred_class", values_to = "probability") %>% 
  # Re-order sentiments for the plot
  mutate(pred_class = factor(pred_class, levels = c(".pred_Positive", ".pred_Negative", ".pred_Neutral"))) %>%
  # get 25 random sentences
  filter(id %in% sample(text_train$id, size = 25)) %>% 
  mutate(id = factor(id)) %>% 
  ggplot(aes(x = id, y = probability, fill = pred_class)) +
  geom_col() +
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  coord_flip() +
  ggtitle("Predicted sentiment probabilities", subtitle = "25 random example sentences")
  

```



# Extra slides




