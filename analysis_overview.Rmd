---
title: "Sentiment Analysis"
author: "Tero Jalkanen"
date: "`r Sys.Date()`"
output: ioslides_presentation
params:
  filter_duplicates: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

## Packages -----------

library(readxl)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(tidytext)
library(scales)
library(textdata)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(tidymodels)
library(topicmodels)
library(textrecipes)
# methods for imbalanced data sets
library(themis)
library(hardhat)
library(glmnet)
library(stopwords)

## Set file paths
here::i_am("analysis_overview.Rmd")

## Set graphical theme
theme_set(theme_minimal())

```

```{r data-download}

## Get the data
text_df <- readxl::read_excel(path = here("data", "sentences_with_sentiment.xlsx")) %>% 
  # move sentiments under same column
  mutate(sentiment = case_when(
    Positive == 1 ~ "Positive",
    Negative == 1 ~ "Negative",
    Neutral == 1 ~ "Neutral",
    # in case of missing sentiment
    TRUE ~ "missing"
  ), .after = ID) %>% 
  # all column names to lowercase
  janitor::clean_names()

```

```{r data-wrangle}

## Find duplicate sentences ----

# indices
double_ind <- duplicated(text_df$sentence)

# The actual sentences
doubles <- text_df$sentence[double_ind] %>% 
  unique()


```


## Introduction

* Our task is to analyze document containing `r length(text_df$sentence)` sentences written in English

* The sentences have been labelled by Subject Matter Experts (SMEs) as being `positive`, `negative` or `neutral` in tone

* We want to explore the data and see if it would be possible to automate the labeling task

```{r}
# Print out label frequencies to show class (im)balance
text_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(percentage = 100*n/length(text_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")

```


# Exploring the data

## Example sentences

* The sentences are picked from regulatory assessments (a few examples):

<!---
Print out a few example sentences
--->

  - *`r doubles[1]`*
  
  - *`r doubles[2]`*
    
  - *`r doubles[3]`*

* There are `r length(doubles)` sentences which appear at least twice in the data set

## Sentiment balance without duplicates

* The duplicated sentences do not appear to have multiple labels assigned to them

* Having the same sentence with the same sentiment label twice does not add new information

```{r}

# sentiments without duplicates
no_doubles_df <- text_df %>%
  select(-id) %>% 
  # include doubles only once
  unique.data.frame()


#balance table
no_doubles_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(percentage = 100*n/length(no_doubles_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences (duplicates removed)")


## Filter out duplicate sentences if the filtering parameter is set to TRUE
if(params$filter_duplicates){
  # remove duplicate entries
  # each sentence only appears once
  text_df <- text_df[!double_ind,]
}


```

* `r length(text_df$sentence)` unique sentences were left for further analyses

# Basics of analyzing text

## How do we make text suitable for ML?

* Human readable text needs to be processed before it can be fed to a machine learning model

* **Tokenization** is the process of splitting text input into smaller (meaningful) units (**tokens**), such as letters, words, sentences, etc.

* Some modifications, such as transforming words into lowercase and removing punctuation, are usually implemented to make sure the model understands that, e.g. `Cabin` and `cabin.` are the same concept

* Once we have extracted the *tokens* from raw text, we can analyze their properties, such as how often they appear

## Most common words

```{r}

# A tidy version of the data with words as tokens
tidy_df <- text_df %>% 
  unnest_tokens(word, sentence)

## Print out a table of the most common words
tidy_df %>% 
  # count occurence of words
  count(word, sort = TRUE) %>%
  head(8) %>% 
  knitr::kable(caption = "Eight most common words in the example sentences")

```

## Removing stop words

* Not all words are equally informative

* Many common words in the English language, such as `the`, `a`, `with` etc. do not necessarily carry much meaningful information

* These words are usually referred to as stop words, and it is common practice to remove them in many Natural Language Processing (NLP) tasks

* Note that there might be big differences between different pre-compiled stop word lists

```{r}

# Get a dictionary of common stopwords which ship with tidytext
data("stop_words")

# remove stop_words
tidy_df <- tidy_df %>% 
  anti_join(stop_words)

```

## What are the most common words?

```{r, fig.height = 6.5, fig.width = 9}

## Most common words visualization

tidy_df %>% 
  # count words after stop words removal
  count(word, sort = TRUE) %>% 
  head(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most common words in the example sentences",
          subtitle = "Excluding common stopwords")
```


## Words vs. different sentiments


```{r, fig.height=7, fig.width=8, warning=FALSE}

tidy_df %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  # twenty most common words per sentiment
  slice(1:20) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"),
                   max.words = 60)

```

## Word frequencies by sentiment

* Mere word counts might not be informative when there is imbalance between sentiment labels

```{r, warning=FALSE}

# Visualize word frequency by sentiment
tidy_df %>% 
  count(sentiment, word) %>% 
  group_by(sentiment) %>% 
  # Calculate word proportions
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  # move each sentiment as separate column
  pivot_wider(names_from = sentiment, values_from = proportion) %>% 
  # move negative and neutral proportions back to the same column
  pivot_longer(Negative:Neutral, names_to = "sentiment", values_to = "proportion") %>% 
  # visualize negative and neutral proportions agains positive proportion
  ggplot(aes(x = proportion, y = Positive, color = abs(Positive - proportion))) +
  geom_abline(lty = 2) +
  geom_point(size = 2.5, alpha = 0.5) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), 
                       low = "darkslategray4", high = "gray75") +
  facet_grid(sentiment ~ .) +
  labs(x = "Proportion in Negative (top) / Neutral (bottom)", y = "Proportion in Positive") +
  theme_bw() +
  theme(legend.position = "none")

```


## Sentiments from dictionaries

* A number of readily available sentiment dictionaries are available, which help us attach emotion to single words

* In dictionary-based methods the total sentiment for a piece of text is determined by adding up the individual sentiment scores for each word in the text

* Example ([AFINN](http://www2.imm.dtu.dk/pubdb/pubs/6010-full.html) dictionary by Finn Årup Nielsen):

```{r}

get_sentiments("afinn") %>% 
  sample_n(4) %>% 
  knitr::kable(caption = "Sentiment scores for four example words")

```

## Attach sentiment to sentences

```{r, fig.height = 4, fig.width = 8, message=FALSE}

tidy_df %>% 
  # Add sentiment scores for words from afinn dictionary 
  inner_join(get_sentiments("afinn"), by = "word") %>% 
  count(id, sentiment, value) %>% 
  group_by(id, sentiment) %>%
  # Calculate sentiment scores for sentences by adding up the product of individual words and their sentiment score
  summarise(sentiment_score = sum(value*n)) %>% 
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral"))) %>% 
  ggplot(aes(x = id, y = sentiment_score, fill = sentiment)) +
  geom_col() +
  ggtitle("Sentiments from AFINN dictionary by Finn Årup Nielsen") +
  # Change plot colors
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  labs(y = "Sentiment scores for sentences", x = "Sentence ID")

```

* Drawbacks such as many words are missing from this dictionary

* Sentiments depend on the context:

> Tero likes to **share** with his friends.

> The OmniCorp **share** went into sharp decline.

# Moving beyond word counts

## Utilizing token frequency

* Term frequency (TF): How frequently a word (etc.) appears in a document

* Inverse document frequency (IDF): decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents:

$$
IDF(\text{term}) = \ln \left( \frac{n_{\text{documents}}}{n_{\text{documents containing term}}} \right) 
$$

* TF-IDF: The product of TF and IDF is a statistic intended to measure how important a word is to a collection of text documents. 

* E.g. a word which only appears in few sentences, but many times will get a higher TF-IDF, whereas words appearing in all sentences get a zero value

## High TF-IDF words by sentiment

```{r, fig.height = 6.5}

# Calculate TF-IDF for words
sentiments_td_idf <- tidy_df %>%
  group_by(sentiment) %>% 
  count(word) %>%
  #filter out numerics
  filter(!grepl(pattern = "[0-9]+", x = word)) %>%
  bind_tf_idf(word, sentiment, n)

# Visualize results
sentiments_td_idf %>% 
  group_by(sentiment) %>% 
  slice_max(tf_idf, n = 5) %>%
  ungroup() %>%
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral"))) %>%
  ggplot(aes(x = tf_idf, y = fct_reorder(word, tf_idf), fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  facet_wrap(~sentiment, ncol = 1, scales = "free") +
  labs(x = "tf-idf", y = NULL) +
  ggtitle("Words with highest TF-IDF by sentiment", subtitle = "Numbers have been removed")
  
```

# n-grams

## Definition and relevance

* A successive group of *n* items (words, syllables, letters etc.) is called an n-gram. Usually refers to words, *e.g.* bigram for two-word combinations, trigram for three and so on

```{r}

# Find bigrams
bigrams_df <- text_df %>% 
  unnest_tokens(bigram, sentence, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram))

bigrams_df %>% 
  count(bigram, sort = TRUE) %>% 
  head(6) %>% 
  knitr::kable(caption = "Six most common bigrams for the data")



```

## Most common bigrams (excluding stop words)

```{r}

# separate bigrams into different columns
bigrams_separated <- bigrams_df %>%
  separate(bigram, c("word1", "word2"), sep = " ")

# Filter out bigrams containing stop words
bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# table
bigram_counts %>% 
  head(10) %>% 
  knitr::kable(caption = "Ten most common bigrams")

```

## Exploring odd combinations

```{r}

text_df %>% 
  # Find sentences containing bigrams starting with "sof"
  filter(grepl(pattern = "sof", x = sentence, ignore.case =TRUE)) %>% 
  select(sentence) %>% 
  head(5) %>% 
  knitr::kable(caption = "Example sentences with the term \"sof\"")

```

## Visualize bigrams by sentiment

```{r, fig.height = 6.5, fig.width=8}

# Unite bigrams after removing stopwords
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

# Count TF-IDF for bigrams 
bigram_tf_idf <- bigrams_united %>%
  count(sentiment, bigram) %>%
  bind_tf_idf(bigram, sentiment, n) %>%
  arrange(desc(tf_idf))

# Visualize by sentiment
bigram_tf_idf %>% 
  # Re-order sentiments for the plot
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Negative", "Neutral"))) %>%
  group_by(sentiment) %>% 
  arrange(desc(tf_idf)) %>%
  top_n(3) %>% 
  ggplot(aes(x = tf_idf, y = bigram, fill = sentiment)) +
  geom_col() +
  scale_fill_manual(values = brewer.pal(n = 3, name = "Set2")) +
  facet_grid(sentiment~., scales = "free")

```

# From exploration to classification

## Basic principles

```{r}

# Make test/train split 20/80
# Same fraction of sentiments in both sets
set.seed(22012023)
text_split <- initial_split(text_df, prop = 0.8, strata = sentiment)

text_train <- training(text_split)
text_test <- testing(text_split)

```

* If we use all our data to train a ML model, then we can not objectively test model performance. High risk for overfitting.

* Test/train-split to evaluate models:

    - Here 80/20 split, so that the train set contains `r length(text_train$sentence)` and the test set `r length(text_test$sentence)` sentences
    
    - Cross-validation is used to combat overfitting in the training phase
    
* A useful model should be able to outperform a so-called *null model*, which just classifies all observations to the most commonly observed class (`Positive` in this case)

* We will use the `sentence` column in the data to try to predict `sentiment`

```{r model-preparations}

# a recipe for classification
sentiment_rec <-
  recipe(sentiment ~ sentence, data = text_train)

## Add processing steps
sentiment_rec <- sentiment_rec %>% 
  # Extract words as tokens
  step_tokenize(sentence) %>% 
  # Only keep the 1000 most common tokens
  step_tokenfilter(sentence, max_tokens = 1000) %>%
  # Calculate TF-IDF
  step_tfidf(sentence)

## Add pre-processing steps to a workflow
## Ensures that test data will undergo similar pre-treatment steps without data leakage
sentiment_wf <- workflow() %>%
  add_recipe(sentiment_rec)


### 5-fold cross validation
## k-fold cross-validation
set.seed(234)
sentiment_folds <- vfold_cv(text_train, v = 5)

```

## Baseline performance

```{r}

# null classification model
null_classification <- null_model() %>%
  set_engine("parsnip") %>%
  set_mode("classification")

## evaluate 5-fold CV
null_rs <- workflow() %>%
  add_recipe(sentiment_rec) %>%
  add_model(null_classification) %>%
  fit_resamples(
    sentiment_folds
  )

```


```{r, fig.height=6.5}

# "Train" null model with entire train data and plot confusion matrix
workflow() %>%
  add_recipe(sentiment_rec) %>%
  add_model(null_classification) %>%
  # Fit null model for entire train data
  fit(data = text_train) %>% 
  # find predictions
  augment(text_train) %>%  
  mutate(sentiment = factor(sentiment, levels = c("Positive", "Neutral", "Negative")),
         .pred_class = factor(.pred_class, levels = c("Positive", "Neutral", "Negative"))) %>% 
  conf_mat(sentiment, .pred_class) %>% 
  autoplot("heatmap") +
  ggtitle("Null model predictions on the train data")


```

## Null model metrics

```{r}

## Collect performace
null_rs %>%
  collect_metrics() %>% 
  select(-.config) %>% 
  knitr::kable(caption = "Performace metrics of the null model")

```

## Classification model with regularization

* Regularized linear models are a class of statistical model that can be used in regression and classification tasks

* Lasso classification learns how much of a penalty should be put of some features which helps us select less features for the final model

* As pre-processing steps we extract words as tokens, filter a maximum of 1000 most common tokens, and calculate the TF-IDF statistic for the tokens

```{r lasso-specification}

## Specifying Lasso as the classification model
## We will tune the regularization term lambda to find optimal level of regularization
multi_spec <- multinom_reg(penalty = tune(), mixture = 1) %>%
  set_mode("classification") %>%
  set_engine("glmnet")


### ----

sparse_bp <- default_recipe_blueprint(composition = "dgCMatrix")

## Define workflow for training the model
multi_lasso_wf <- workflow() %>%
  ## Train to predict sentiment from sentence by using sentiment_rec
  add_recipe(sentiment_rec, blueprint = sparse_bp) %>%
  ## Use glmnet Lasso as the model engine
  add_model(multi_spec)


```

