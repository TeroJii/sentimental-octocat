---
title: "Sentiment Analysis"
author: "Tero Jalkanen"
date: "`r Sys.Date()`"
output: ioslides_presentation
params:
  filter_duplicates: TRUE
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

## Packages -----------

library(readxl)
library(here)
library(tidyverse)
library(janitor)
library(skimr)
library(tidytext)
library(scales)
library(textdata)
library(RColorBrewer)
library(wordcloud)
library(reshape2)
library(tidymodels)
library(topicmodels)
library(textrecipes)
# methods for imbalanced data sets
library(themis)
library(hardhat)
library(glmnet)
library(stopwords)

## Set file paths
here::i_am("analysis_overview.Rmd")

## Set graphical theme
theme_set(theme_minimal())

```

```{r data-download}

## Get the data
text_df <- readxl::read_excel(path = here("data", "sentences_with_sentiment.xlsx")) %>% 
  # move sentiments under same column
  mutate(sentiment = case_when(
    Positive == 1 ~ "Positive",
    Negative == 1 ~ "Negative",
    Neutral == 1 ~ "Neutral",
    # in case of missing sentiment
    TRUE ~ "missing"
  ), .after = ID) %>% 
  # all column names to lowercase
  janitor::clean_names()

```

```{r data-wrangle}

## Find duplicate sentences ----

# indices
double_ind <- duplicated(text_df$sentence)

# The actual sentences
doubles <- text_df$sentence[double_ind] %>% 
  unique()


```


## Introduction

* Our task is to analyze document containing `r length(text_df$sentence)` sentences written in English

* The sentences have been labelled by Subject Matter Experts (SMEs) as being `positive`, `negative` or `neutral` in tone

* We want to explore the data and see if it would be possible to automate the labeling task

```{r}
# Print out label frequencies to show class (im)balance
text_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(percentage = 100*n/length(text_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences")

```


# Exploring the data

## Example sentences

* The sentences are picked from regulatory assessments (a few examples):

<!---
Print out a few example sentences
--->

  - *`r doubles[1]`*
  
  - *`r doubles[2]`*
    
  - *`r doubles[3]`*

* There are `r length(doubles)` sentences which appear at least twice in the data set

## Sentiment balance without duplicates

* The duplicated sentences do not appear to have multiple labels assigned to them

* Having the same sentence with the same sentiment label twice does not add new information

```{r}

# sentiments without duplicates
no_doubles_df <- text_df %>%
  select(-id) %>% 
  # include doubles only once
  unique.data.frame()


#balance table
no_doubles_df %>% 
  group_by(sentiment) %>% 
  summarise(n = n()) %>% 
  mutate(percentage = 100*n/length(no_doubles_df$sentiment)) %>% 
  knitr::kable(digits = 2, caption = "The sentiment labels for the example sentences (duplicates removed)")


## Filter out duplicate sentences if the filtering parameter is set to TRUE
if(params$filter_duplicates){
  # remove duplicate entries
  # each sentence only appears once
  text_df <- text_df[!double_ind,]
}


```

* `r length(text_df$sentence)` unique sentences were left for further analyses

# Basics of analyzing text

## How do we make text suitable for ML?

* Human readable text needs to be processed before it can be fed to a machine learning model

* **Tokenization** is the process of splitting text input into smaller (meaningful) units (**tokens**), such as letters, words, sentences, etc.

* Some modifications, such as transforming words into lowercase and removing punctuation, are usually implemented to make sure the model understands that, e.g. `Cabin` and `cabin.` are the same concept

* Once we have extracted the *tokens* from raw text, we can analyze their properties, such as how often they appear

## Most common words

```{r}

# A tidy version of the data with words as tokens
tidy_df <- text_df %>% 
  unnest_tokens(word, sentence)

## Print out a table of the most common words
tidy_df %>% 
  # count occurence of words
  count(word, sort = TRUE) %>%
  head(8) %>% 
  knitr::kable(caption = "Eight most common words in the example sentences")

```

## Removing stop words

* Not all words are equally informative

* Many common words in the English language, such as `the`, `a`, `with` etc. do not necessarily carry much meaningful information

* These words are usually referred to as stop words, and it is common practice to remove them in many Natural Language Processing (NLP) tasks

* Note that there might be big differences between different pre-compiled stop word lists

```{r}

# Get a dictionary of common stopwords which ship with tidytext
data("stop_words")

# remove stop_words
tidy_df <- tidy_df %>% 
  anti_join(stop_words)

```

## What are the most common words?

```{r, fig.height=7}

## Most common words visualization

tidy_df %>% 
  # count words after stop words removal
  count(word, sort = TRUE) %>% 
  head(15) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(x = n, y = word)) +
  geom_col() +
  labs(y = NULL) +
  ggtitle("Most common words in the example sentences",
          subtitle = "Excluding common stopwords")
```


## Words vs. different sentiments


```{r, fig.height=7, fig.width=8, warning=FALSE}

tidy_df %>% 
  count(word, sentiment, sort = TRUE) %>%
  group_by(sentiment) %>%
  # twenty most common words per sentiment
  slice(1:20) %>% 
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = brewer.pal(n = 3, "Pastel1"),
                   max.words = 60)

```

