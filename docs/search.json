[
  {
    "objectID": "words.html#words-as-tokens",
    "href": "words.html#words-as-tokens",
    "title": "2  Extracting and Analyzing Words",
    "section": "2.1 Words as Tokens",
    "text": "2.1 Words as Tokens\nIn order to analyze natural language, the raw text needs to be somehow transformed into a numeric format. One way to do this is to split passages of text into elements of one or more words. This transformation of text into smaller units is called tokenization.\nLet’s test turning a few made-up movie review sentences into separate words.\n\nlibrary(tibble)\nlibrary(tidytext)\nlibrary(magrittr)\n\n# A few example sentences\nmovie_comments <- tibble(id = 1:6,\n       text = c(\"The movie was great.\",\n                \"The movie was bad.\",\n                \"I didn't care for the movie.\",\n                \"The movie was not bad at all.\",\n                \"The beginning of the movie was lousy, but it turned out the be one of the best I've ever seen.\",\n                \"The movie was quite alright.\"))\n\n# Split sentences into separate words\nmovie_comments %>% unnest_tokens(word, text)\n\n# A tibble: 46 × 2\n      id word  \n   <int> <chr> \n 1     1 the   \n 2     1 movie \n 3     1 was   \n 4     1 great \n 5     2 the   \n 6     2 movie \n 7     2 was   \n 8     2 bad   \n 9     3 i     \n10     3 didn't\n# … with 36 more rows\n\n\nThe unnest_tokens() function also strips all the punctuation, white spaces etc., and turns all the words into lowercase.\n\n2.1.1 TF-IDF\nTerm frequency (TF), inverse document frequency (IDF), and their product (TF-IDF) which describes the frequency of the term adjusted for how rarely it is used (Silge and Robinson 2017), are all quantities which can be used for assessing the importance of a word in a passage of text.\n\nThe statistic tf-idf is intended to measure how important a word is to a document in a collection (or corpus) of documents, for example, to one novel in a collection of novels or to one website in a collection of websites.\n\n\\[\nIDF(\\text{term}) = \\ln \\left( \\frac{n_{\\text{documents}}}{n_{\\text{documents containing term}}} \\right)\n\\]"
  },
  {
    "objectID": "words.html#n-grams",
    "href": "words.html#n-grams",
    "title": "2  Extracting and Analyzing Words",
    "section": "2.2 n-grams",
    "text": "2.2 n-grams\nA successive group of n items (words, syllables, letters etc.) is called an n-gram. Usually refers to words, e.g. bigram for two-word combinations, trigram for three.\n\n# Split sentences into bigrams\nmovie_comments %>% \n  unnest_tokens(bigram, text, token = \"ngrams\", n = 2)\n\n# A tibble: 40 × 2\n      id bigram     \n   <int> <chr>      \n 1     1 the movie  \n 2     1 movie was  \n 3     1 was great  \n 4     2 the movie  \n 5     2 movie was  \n 6     2 was bad    \n 7     3 i didn't   \n 8     3 didn't care\n 9     3 care for   \n10     3 for the    \n# … with 30 more rows\n\n\n\n\n\n\nSilge, J., and D. Robinson. 2017. Text Mining with R: A Tidy Approach. O’Reilly Media. https://www.tidytextmining.com/."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "3  Summary",
    "section": "",
    "text": "Summary here…"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Hvitfeldt, E., and J. Silge. 2022. Supervised Machine Learning for\nText Analysis in R. CRC Press. https://smltar.com/.\n\n\nSilge, J., and D. Robinson. 2017. Text Mining with R: A\nTidy Approach. O’Reilly Media. https://www.tidytextmining.com/."
  }
]